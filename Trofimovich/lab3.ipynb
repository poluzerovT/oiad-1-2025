import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import t

# Настройка визуализации
sns.set(style="whitegrid")

df = pd.read_csv('/Users/liker/Downloads/insurance.csv')
df.head()

print(df.isnull().sum())

numeric_cols = df.select_dtypes(include=np.number).columns

for col in numeric_cols:
    plt.figure(figsize=(8, 4))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot для столбца: {col}')
    plt.xlabel(col)
    plt.show()

print(df.dtypes)

categorical_cols = df.select_dtypes(include='object').columns
print("Категориальные признаки:", list(categorical_cols))

from sklearn.preprocessing import LabelEncoder

label_cols = ['sex', 'smoker']
label_encoders = {}

for col in label_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # можно использовать для обратного преобразования

df = pd.get_dummies(df, columns=['region'])
# Корреляционная матрица по всем числовым признакам (корреляции Пирсона)
correlation_matrix = df.corr(numeric_only=True)
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True)
plt.title('Корреляционная матрица признаков')
plt.show()
import numpy as np
from sklearn.metrics import r2_score, mean_squared_error

# w = (XtX)^-1 * Xty

X = df.drop(columns='charges').astype(float).values
y = df['charges'].astype(float).values

X_b = np.hstack([np.ones((X.shape[0], 1)), X])

w = np.linalg.pinv(X_b) @ y.reshape(-1, 1)

y_pred_manual = X_b @ w

r2_analytical = r2_score(y, y_pred_manual.ravel())
mse_analytical = mean_squared_error(y, y_pred_manual.ravel())

print(f"R² (аналитическая модель): {r2_analytical:.4f}")
print(f"MSE (аналитическая модель): {mse_analytical:.2f}")

feature_names = ['intercept'] + list(df.drop(columns='charges').columns)
print("Аналитическая модель линейной регрессии:")
for i, name in enumerate(feature_names):
    print(f"{name:20s}: {w[i][0]:.4f}")

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error

X = df.drop(columns='charges').astype(float).values
y = df['charges'].astype(float).values.reshape(-1, 1)

X_b = np.hstack([np.ones((X.shape[0], 1)), X])

m, n = X_b.shape
w = np.zeros((n, 1))
learning_rate = 1e-4
n_iterations = 100000

for i in range(n_iterations):
    y_pred = X_b @ w
    error = y_pred - y
    gradients = (2 / m) * X_b.T @ error
    w -= learning_rate * gradients

y_pred_gd = X_b @ w

r2_gd = r2_score(y, y_pred_gd)
mse_grad = mean_squared_error(y, y_pred_gd)

print(f"R² (градиентный спуск): {r2_gd:.4f}")
print(f"MSE (градиентный спуск): {mse_grad:.2f}")

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error

X = df.drop(columns='charges').astype(float).values
y = df['charges'].astype(float).values.reshape(-1, 1)
X_b = np.hstack([np.ones((X.shape[0], 1)), X])

m, n = X_b.shape
learning_rate = 1e-4
n_iterations = 100000

lambda_values = [0, 0.001, 0.01, 0.1, 1, 10]

best_mse = float('inf')
best_r2 = None
best_lambda = None

print(f"{'λ':>7} | {'R²':>8} | {'MSE':>12}")
print("-" * 32)

for lambda_ in lambda_values:
    w = np.zeros((n, 1))

    for i in range(n_iterations):
        y_pred = X_b @ w
        error = y_pred - y

        w_reg = w.copy()
        w_reg[0] = 0

        gradients = (2 / m) * X_b.T @ error + 2 * lambda_ * w_reg
        w -= learning_rate * gradients

    y_pred_gd = X_b @ w
    r2_gd = r2_score(y, y_pred_gd)
    mse = mean_squared_error(y, y_pred_gd)

    print(f"{lambda_:7.3f} | {r2_gd:8.4f} | {mse:12.2f}")

    if mse < best_mse:
        best_mse = mse
        best_r2 = r2_gd
        best_lambda = lambda_

mse_grad_reg = best_mse
r2_grad_reg = best_r2

print(f"\nЛучшая MSE среди λ: {mse_grad_reg:.2f} при λ = {best_lambda}")
print(f" Соответствующий R²: {r2_grad_reg:.4f}")
import numpy as np
from sklearn.metrics import r2_score, mean_squared_error

X = df.drop(columns='charges').astype(float).values
y = df['charges'].astype(float).values.reshape(-1, 1)
X_b = np.hstack([np.ones((X.shape[0], 1)), X])
m, n = X_b.shape

lambda_values = [0, 0.001, 0.01, 0.1, 1, 10]
feature_names = ['intercept'] + list(df.drop(columns='charges').columns)

best_mse = float('inf')
best_r2 = None
best_lambda = None

for lambda_ in lambda_values:
    XtX = X_b.T @ X_b
    XtY = X_b.T @ y

    reg_matrix = np.eye(n)
    reg_matrix[0, 0] = 0

    w_ridge = np.linalg.inv(XtX + lambda_ * reg_matrix) @ XtY
    y_pred_ridge = X_b @ w_ridge
    r2_ridge = r2_score(y, y_pred_ridge.ravel())
    mse = mean_squared_error(y, y_pred_ridge)

    print(f"\nλ = {lambda_:>5} → R² = {r2_ridge:.4f}, MSE = {mse:.2f}")
    print("Коэффициенты модели:")
    for i, name in enumerate(feature_names):
        print(f"{name:20s}: {w_ridge[i][0]:.4f}")

    if mse < best_mse:
        best_mse = mse
        best_r2 = r2_ridge
        best_lambda = lambda_

mse_analytical_reg = best_mse
r2_analytical_reg = best_r2

print(f"\nЛучшая MSE среди λ: {mse_analytical_reg:.2f} при λ = {best_lambda}")
print(f"Соответствующий R²: {r2_analytical_reg:.4f}")

import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

y = df['charges'].astype(float).values.reshape(-1, 1)
y_mean = np.mean(y)
y_pred_const = np.full_like(y, y_mean)

mse_const = mean_squared_error(y, y_pred_const)
r2_const = r2_score(y, y_pred_const)

print(f"MSE (константная модель): {mse_const:.2f}")
print(f"R²  (константная модель): {r2_const:.4f}")

print("\nСравнение моделей:")
print(f"{'Модель':35s} | {'MSE':>12s} | {'R²':>8s}")
print("-" * 62)
print(f"{'1. Константная':35s} | {mse_const:12.2f} | {r2_const:8.4f}")
print(f"{'2. Аналитическая (без регуляризации)':35s} | {mse_analytical:12.2f} | {r2_analytical:8.4f}")
print(f"{'3. Аналитическая (λ=0.001)':35s} | {mse_analytical_reg:12.2f} | {r2_ridge:8.4f}")
print(f"{'4. Градиентный спуск (без регуляризации)':35s} | {mse_grad:12.2f} | {r2_gd:8.4f}")
print(f"{'5. Градиентный спуск (λ=0.1)':35s} | {mse_grad_reg:12.2f} | {r2_grad_reg:8.4f}")
